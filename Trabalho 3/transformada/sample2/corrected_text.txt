. {ees mmAcn0N8. ON OWLEDNE 0 DATA NOREEN VEL 1. 10.1, MANYFEBRY 200
Natural Language Grammatical Inference
with Recurrent Neural Networks
Steve Lawrence, Member, JEEE, C. Lee Giles, Fellow, IEEE, and Sandiway Fong
panrechTe pr umes tudinal aconpls gant wh co tea petly bea sed
a ae ere tay ncralojagy arto: partake o copunancl foray anion oe ta oe
eee ate wine Prep oa Pantawr apne tones @ Gonrewrt nes Beary nosy, Ne
See rena siacireieed es was crgrara enredhy Goma naman oars be ame
jit cpsien ss not poorascouegemeaiel dan, Nort weer rool ah Gd pasts ROSE
Se eer ener Seer eeat pp teama samcoom or Sarat Te pases Danie a
See er Tae nr cen ene goomer ae room bes ea roe ne opens sat
eae ae eee cmrpans ate poset cn mcepluanvcaprins Yon oars op teat
esis cere aneats tans: can canoes ov tens sae sao cope pene Te egoen oh
SSre'ranr nase ped Sac cums cotta bent tie i ea sige
i  e
pecans gravalepenarmperireloena-weprigermatanr
ss ana encg epIaMSa
1 IntropucTiON
TT re ose tl ning natant ann option te ne
fn ino"lamcd vt ina components steed by trata forthe tack aeeped Sten proves oid
speakers on sharply grammatical/ungrammatical data. ence and describes the data. Section 4 lists the recurrent
Grip recent novel networks are investigated for neural network made invenigned and provides ete
computational reasons. Computationally, recurrent neural the data encoding for the networks. Section 5 presents the
networks are more powerful than feedforward networks results of iwestigation into various training heuristics and
Jeast Turing equivalent [53], [54]. We investigate the presents the main results and simulation details and
ee ne ees a
architectures, in particular Elman, Narendra and Parthasar- of rules in the form of deterministic finite state automata is
Sniy (NGM), and Willan ed Zips OWEZ) Ure qreipated Seton Jad Secon 8 poems session
oe ee
Toor nctcet. Wend ut bt Ean sed Wz eure
her implementing technics for ueprong the conver. 2 MOTIVATION
ee eee ee
Traagruune Sning deren Weanajor he Opeaton Manurt capeees her estealy ben hand xing
of the networks and investigate a rule approximation of symbolic computation and recursive processes. The most
Shot near newect hes amet "pecaly me mba cemutaion and recunine pres, The most
<atracton of resin the form of deterministic Fite sate Faiesatedexceptions such as grams ot huden Matkow
— models. However, finite-state models cannot represent
Previous work (38] has compared neural networks With hierarchical structures as found in natural language’ (48).
other machine learning, paradigms on this problem —this {nthe past few years, several recurrent neural network
Ichnehures ave emerged whch have been wed fae
ee ee ee ee
" [sary apenas fetid omar
‘0 The authors ure vwih NEC Research Dustitute. 4 Independence Way, language problems, e.g... papers using the Elman network
Princeton, NP OSH for natural language tasks inctude: [1], {12}, (24). (58), (99)
